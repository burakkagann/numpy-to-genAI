.. _module-9-2-3-recurrent-networks-sequences:

=========================================
9.2.3 Recurrent Networks for Sequences
=========================================

:Duration: 35-40 minutes
:Level: Intermediate-Advanced
:Prerequisites: 9.1.1 (Perceptron from Scratch)

Overview
========

Recurrent Neural Networks (RNNs) are a class of neural networks designed to process sequential data by maintaining a form of memory. Unlike feedforward networks that treat each input independently, RNNs pass information from one step to the next, allowing them to capture patterns that unfold over time.

In this exercise, you will implement a simple RNN from scratch using NumPy to generate color sequences for generative art. The RNN will learn to produce smooth, aesthetically pleasing color transitions by maintaining a hidden state that remembers previous colors. This application demonstrates the core concept of recurrent networks: the ability to use past information when making current predictions.

Jeffrey Elman introduced the simple recurrent network in 1990, demonstrating that networks with feedback connections could learn temporal patterns in sequences [Elman1990]_. Since then, recurrent architectures have become fundamental to sequence modeling tasks in areas ranging from natural language processing to music generation [LeCun2015b]_. The deep learning revolution has seen RNNs applied to creative domains including text generation and generative art [Galanter2016b]_. By building one from scratch, you will develop deep intuition for how sequential memory works in neural networks.

Learning Objectives
-------------------

By the end of this exercise, you will be able to:

* Understand RNN architecture with input, hidden state, and output
* Implement the recurrent forward pass that maintains memory across time steps
* Generate sequences autoregressively by feeding outputs back as inputs
* Visualize how hidden state enables smooth color transitions


Quick Start: See It In Action
=============================

Run this code to generate color sequences using a recurrent neural network:

.. code-block:: python
   :caption: Generate color sequences with an RNN
   :linenos:

   import numpy as np
   from PIL import Image

   class SimpleRNN:
       def __init__(self, input_size=3, hidden_size=16, output_size=3):
           np.random.seed(42)
           self.W_xh = np.random.randn(input_size, hidden_size) * 0.3
           self.W_hh = np.eye(hidden_size) * 0.5 + np.random.randn(hidden_size, hidden_size) * 0.1
           self.W_hy = np.random.randn(hidden_size, output_size) * 0.4
           self.b_h = np.zeros(hidden_size)
           self.b_y = np.array([0.5, 0.5, 0.5])
           self.hidden_size = hidden_size

       def forward_step(self, x, h_prev):
           h_new = np.tanh(np.dot(x, self.W_xh) + np.dot(h_prev, self.W_hh) + self.b_h)
           y = np.clip(np.dot(h_new, self.W_hy) + self.b_y, 0.0, 1.0)
           return y, h_new

       def generate_sequence(self, seed_color, length=20):
           h = np.zeros(self.hidden_size)
           colors = [np.array(seed_color)]
           current = seed_color
           for _ in range(length - 1):
               next_color, h = self.forward_step(current, h)
               colors.append(next_color)
               current = next_color
           return np.array(colors)

   rnn = SimpleRNN()
   colors = rnn.generate_sequence([1.0, 0.2, 0.2], length=25)
   print(f"Generated {len(colors)} colors starting from red")

.. figure:: rnn_output.png
   :width: 500px
   :align: center
   :alt: Five color sequences generated by the RNN, showing smooth transitions from different seed colors

   Color sequences generated by the RNN from different starting colors. Each row shows a sequence of 25 colors evolving smoothly over time, demonstrating how the hidden state creates continuity.

The RNN generates smooth color transitions because its hidden state "remembers" previous colors. Each output color depends not just on the current input, but on the accumulated history encoded in the hidden state. This is fundamentally different from a feedforward network that would treat each color independently. The implementation uses NumPy for efficient array operations [NumPyDocs923]_ and Pillow for image processing [PillowDocs923]_.


Core Concepts
=============

Concept 1: What Are Recurrent Networks?
---------------------------------------

A **Recurrent Neural Network (RNN)** is a neural network designed to process sequences by maintaining an internal state that carries information from previous steps. This state acts as a form of memory, allowing the network to recognize patterns that span multiple time steps.

Consider processing a sequence of colors to generate the next color in a palette:

* A **feedforward network** would only see the current color, ignoring what came before
* A **recurrent network** maintains a hidden state that encodes the history of previous colors

The key difference is the **feedback connection**: the hidden state from the previous time step becomes part of the input for the current step. This creates a loop in the network architecture.

.. figure:: rnn_architecture.png
   :width: 600px
   :align: center
   :alt: RNN cell architecture showing input, hidden state with feedback loop, and output

   The RNN cell architecture. The hidden state h_t receives input from both the current input x_t and the previous hidden state h_{t-1}, creating a feedback loop that enables memory.

**Why recurrence matters for sequences:**

* **Text**: Understanding "not good" requires remembering "not" when processing "good"
* **Music**: A melody is defined by the sequence of notes, not individual notes
* **Color palettes**: Aesthetic transitions depend on what colors came before
* **Animation**: Smooth motion requires consistency with previous frames

.. admonition:: Did You Know?

   Jeffrey Elman's 1990 paper "Finding Structure in Time" introduced the simple recurrent network and demonstrated that it could learn grammatical structure from sequences of words without being explicitly taught the rules [Elman1990]_. This was a breakthrough in showing that neural networks could discover patterns in temporal data.


Concept 2: The Recurrent Cell
-----------------------------

The heart of an RNN is the **recurrent cell**, which computes a new hidden state by combining the current input with the previous hidden state. The mathematical formulation is:

.. code-block:: text

   h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)
   y_t = W_hy * h_t + b_y

Where:

* **x_t** is the input at time step t (e.g., an RGB color)
* **h_{t-1}** is the hidden state from the previous step (the memory)
* **h_t** is the new hidden state after processing the input
* **y_t** is the output at time step t
* **W_xh, W_hh, W_hy** are weight matrices
* **b_h, b_y** are bias vectors
* **tanh** is the hyperbolic tangent activation function

Here is the implementation:

.. code-block:: python
   :caption: The recurrent forward pass
   :linenos:
   :emphasize-lines: 8,9,13

   def forward_step(self, x, h_prev):
       """
       Perform one step of the RNN forward pass.

       x: current input (shape: input_size)
       h_prev: previous hidden state (shape: hidden_size)
       """
       # Combine input with memory from previous step
       h_new = np.tanh(
           np.dot(x, self.W_xh) +      # Current input contribution
           np.dot(h_prev, self.W_hh) +  # Memory contribution
           self.b_h                      # Bias
       )

       # Generate output from hidden state
       y = np.dot(h_new, self.W_hy) + self.b_y
       y = np.clip(y, 0.0, 1.0)  # Keep colors valid

       return y, h_new

**Line 8-12**: The hidden state update combines two sources of information:

* ``np.dot(x, self.W_xh)``: How the current input affects the hidden state
* ``np.dot(h_prev, self.W_hh)``: How the previous state affects the new state

**Line 16**: The output is computed from the hidden state using a separate weight matrix.

The **tanh activation** squashes values to the range [-1, 1], preventing the hidden state from growing unbounded. This is crucial for stability over long sequences [Goodfellow2016b]_.

.. figure:: sequence_unrolling.png
   :width: 700px
   :align: center
   :alt: RNN unrolled through time showing information flow from x1 to xn with hidden states connected horizontally

   The RNN "unrolled" through time. Each time step processes an input, updates the hidden state, and produces an output. The horizontal arrows show how information flows through the hidden states across time steps.

.. important::

   The hidden state h is the "memory" of the RNN. It encodes a summary of all previous inputs the network has seen. For color sequences, this means the RNN can learn that after warm colors, cool colors create pleasing contrast, or that gradual transitions are more aesthetic than abrupt changes.


Concept 3: Sequence Generation
------------------------------

To generate new sequences, we use **autoregressive generation**: the output from one step becomes the input for the next step. This creates a feedback loop where the network generates content based on its own previous outputs [Graves2013]_. This technique has been popularized through applications like character-level text generation [Karpathy2015]_.

.. code-block:: python
   :caption: Autoregressive sequence generation
   :linenos:
   :emphasize-lines: 6,11,13,14

   def generate_sequence(self, seed_color, length=20):
       """
       Generate a sequence of colors starting from a seed.
       """
       # Initialize hidden state with zeros (no prior memory)
       h = np.zeros(self.hidden_size)

       colors = [np.array(seed_color)]
       current_color = seed_color

       for step in range(length - 1):
           # Generate next color based on current state
           next_color, h = self.forward_step(current_color, h)
           colors.append(next_color)
           current_color = next_color  # Feed output back as input

       return np.array(colors)

**Line 6**: The hidden state starts at zero, meaning no prior memory. The first generated color depends only on the seed color.

**Line 11-14**: Each iteration generates the next color and updates the hidden state. Crucially, ``current_color = next_color`` creates the autoregressive loop: the output becomes the next input.

**The role of hidden size:**

The hidden size determines the network's memory capacity. A larger hidden state can store more complex patterns but requires more computation:

.. figure:: hidden_size_comparison.png
   :width: 550px
   :align: center
   :alt: Comparison of color sequences generated with hidden sizes 8, 16, 32, and 64

   Effect of hidden size on generated sequences. Larger hidden states can capture more complex color relationships, while smaller states produce simpler, more repetitive patterns.

.. note::

   **Vanishing Gradients**: Training RNNs with backpropagation through time (BPTT) can cause gradients to vanish or explode over long sequences [Pascanu2013]_. This is why architectures like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU) were developed, using gating mechanisms to better preserve gradients [Hochreiter1997]_. In this exercise, we use pre-set weights to focus on the forward pass concepts rather than training.


Hands-On Exercises
==================

Exercise 1: Execute and Explore
-------------------------------

Run the complete RNN implementation and observe how it generates color sequences.

.. code-block:: python
   :caption: rnn_sequence_generator.py
   :linenos:

   import numpy as np
   from PIL import Image

   class SimpleRNN:
       def __init__(self, input_size=3, hidden_size=16, output_size=3):
           np.random.seed(42)
           self.input_size = input_size
           self.hidden_size = hidden_size
           self.output_size = output_size

           # Weight matrices
           self.W_xh = np.random.randn(input_size, hidden_size) * 0.3
           self.W_hh = np.eye(hidden_size) * 0.5 + np.random.randn(hidden_size, hidden_size) * 0.1
           self.W_hy = np.random.randn(hidden_size, output_size) * 0.4
           self.b_h = np.zeros(hidden_size)
           self.b_y = np.array([0.5, 0.5, 0.5])

       def forward_step(self, x, h_prev):
           h_new = np.tanh(np.dot(x, self.W_xh) + np.dot(h_prev, self.W_hh) + self.b_h)
           y = np.clip(np.dot(h_new, self.W_hy) + self.b_y, 0.0, 1.0)
           return y, h_new

       def generate_sequence(self, seed_color, length=20):
           h = np.zeros(self.hidden_size)
           colors = [np.array(seed_color)]
           current = seed_color
           for _ in range(length - 1):
               next_color, h = self.forward_step(current, h)
               colors.append(next_color)
               current = next_color
           return np.array(colors)

   # Create RNN and generate sequences
   rnn = SimpleRNN(hidden_size=16)

   # Try different seed colors
   red_sequence = rnn.generate_sequence([1.0, 0.2, 0.2], length=10)
   blue_sequence = rnn.generate_sequence([0.2, 0.2, 1.0], length=10)

   print("Red seed sequence (first 5 colors):")
   for i, color in enumerate(red_sequence[:5]):
       print(f"  Step {i}: RGB({color[0]:.2f}, {color[1]:.2f}, {color[2]:.2f})")

   print("\nBlue seed sequence (first 5 colors):")
   for i, color in enumerate(blue_sequence[:5]):
       print(f"  Step {i}: RGB({color[0]:.2f}, {color[1]:.2f}, {color[2]:.2f})")

After running the code, answer these reflection questions:

1. What happens to the hidden state after processing each color?
2. How does the sequence starting from red differ from the one starting from blue?
3. Why do we initialize the hidden state to zeros?
4. What would happen if we used a very large hidden size (e.g., 256)?

.. dropdown:: Answers and Explanation
   :class-title: sd-font-weight-bold

   1. **Hidden state evolution**: After each step, the hidden state is updated to encode information about the current and all previous inputs. It acts as a compressed summary of the sequence history. You can observe this by printing ``h`` values across steps.

   2. **Different seeds, different sequences**: The red and blue seeds produce different sequences because the initial color influences the first hidden state update, which propagates through the entire sequence. The RNN "remembers" what it started with.

   3. **Zero initialization**: We start with zeros to indicate "no prior memory." This means the first color is processed with no context from previous steps. For the first step, only ``W_xh * x`` contributes to the hidden state (since ``W_hh * 0 = 0``).

   4. **Large hidden size**: A larger hidden size (256) would give the network more memory capacity but wouldn't necessarily produce better results with our simple pre-set weights. It would also be slower to compute. For complex sequences (like text), larger hidden sizes can capture more nuanced patterns.


Exercise 2: Modify Parameters
-----------------------------

Experiment with the RNN parameters to understand their effects on color generation.

**Goal 1**: Change the hidden size

Modify the hidden size and observe how it affects the generated sequences:

.. code-block:: python
   :caption: Experiment with different hidden sizes

   # Small hidden size (limited memory)
   rnn_small = SimpleRNN(hidden_size=4)
   colors_small = rnn_small.generate_sequence([0.8, 0.3, 0.5], length=20)

   # Medium hidden size (default)
   rnn_medium = SimpleRNN(hidden_size=16)
   colors_medium = rnn_medium.generate_sequence([0.8, 0.3, 0.5], length=20)

   # Large hidden size (more capacity)
   rnn_large = SimpleRNN(hidden_size=64)
   colors_large = rnn_large.generate_sequence([0.8, 0.3, 0.5], length=20)

**Goal 2**: Modify the weight initialization

Change the weight scale to see how it affects stability:

.. code-block:: python
   :caption: Different weight scales

   class RNNWithScale(SimpleRNN):
       def __init__(self, hidden_size=16, weight_scale=0.3):
           np.random.seed(42)
           self.hidden_size = hidden_size
           self.W_xh = np.random.randn(3, hidden_size) * weight_scale
           self.W_hh = np.eye(hidden_size) * 0.5 + np.random.randn(hidden_size, hidden_size) * (weight_scale / 3)
           self.W_hy = np.random.randn(hidden_size, 3) * weight_scale
           self.b_h = np.zeros(hidden_size)
           self.b_y = np.array([0.5, 0.5, 0.5])

   # Small weights (more stable)
   rnn_stable = RNNWithScale(weight_scale=0.1)

   # Larger weights (more dynamic)
   rnn_dynamic = RNNWithScale(weight_scale=0.5)

**Goal 3**: Generate longer sequences

Observe what happens over extended generation:

.. code-block:: python
   :caption: Generating longer sequences

   rnn = SimpleRNN(hidden_size=16)

   # Short sequence
   short = rnn.generate_sequence([1.0, 0.5, 0.0], length=10)

   # Long sequence
   long = rnn.generate_sequence([1.0, 0.5, 0.0], length=100)

   print(f"Short sequence variation: {np.std(short):.4f}")
   print(f"Long sequence variation: {np.std(long):.4f}")

.. dropdown:: Hints and Solutions
   :class-title: sd-font-weight-bold

   **Goal 1 observations**:

   * Small hidden size (4): Sequences may converge quickly to a stable color because limited memory cannot maintain complex dynamics
   * Medium hidden size (16): Good balance of variety and coherence
   * Large hidden size (64): Potentially more varied patterns, but with our simple weights, may not differ dramatically

   **Goal 2 observations**:

   * Small weights (0.1): Colors change slowly, more subtle transitions
   * Large weights (0.5): More dramatic color changes, potentially less stable

   **Goal 3 observations**:

   * Long sequences often converge toward an attractor state due to the tanh activation keeping values bounded. With the identity component in W_hh, our RNN maintains some stability.


Exercise 3: Create Your Own
---------------------------

Complete the starter code to implement your own RNN for color sequence generation.

**Requirements**:

* Implement the ``forward_step`` method with proper weight matrix operations
* Implement the ``generate_sequence`` method with hidden state management
* Generate and visualize a color sequence

**Starter Code**:

.. code-block:: python
   :caption: rnn_starter.py (complete the TODO sections)
   :linenos:

   import numpy as np
   from PIL import Image

   class MyRNN:
       def __init__(self, hidden_size=16):
           np.random.seed(42)
           self.hidden_size = hidden_size

           # TODO: Initialize weight matrices
           # W_xh: input (3) to hidden (hidden_size)
           # W_hh: hidden to hidden (hidden_size x hidden_size)
           # W_hy: hidden to output (3)
           self.W_xh = None  # Replace with: np.random.randn(3, hidden_size) * 0.3
           self.W_hh = None  # Replace with: np.eye(hidden_size) * 0.5 + np.random.randn(...) * 0.1
           self.W_hy = None  # Replace with: np.random.randn(hidden_size, 3) * 0.4
           self.b_h = np.zeros(hidden_size)
           self.b_y = np.array([0.5, 0.5, 0.5])

       def forward_step(self, x, h_prev):
           """
           Compute one step of the RNN.

           x: input color (3,)
           h_prev: previous hidden state (hidden_size,)

           Returns: (output_color, new_hidden_state)
           """
           # TODO: Compute new hidden state
           # h_new = tanh(W_xh * x + W_hh * h_prev + b_h)
           h_new = None  # Replace

           # TODO: Compute output
           # y = W_hy * h_new + b_y, then clip to [0, 1]
           y = None  # Replace

           return y, h_new

       def generate_sequence(self, seed_color, length=20):
           """Generate a sequence of colors starting from seed."""
           # TODO: Initialize hidden state to zeros
           h = None  # Replace

           colors = [np.array(seed_color)]
           current = seed_color

           for _ in range(length - 1):
               # TODO: Generate next color and update hidden state
               # next_color, h = self.forward_step(current, h)
               pass  # Replace

               # TODO: Append to colors list and update current
               pass  # Replace

           return np.array(colors)

   # Test your implementation
   rnn = MyRNN(hidden_size=16)
   sequence = rnn.generate_sequence([0.8, 0.2, 0.3], length=15)
   print(f"Generated {len(sequence)} colors")

.. dropdown:: Hint 1: Weight Initialization
   :class-title: sd-font-weight-bold

   For the weight matrices in ``__init__``:

   .. code-block:: python

      self.W_xh = np.random.randn(3, hidden_size) * 0.3
      self.W_hh = np.eye(hidden_size) * 0.5 + np.random.randn(hidden_size, hidden_size) * 0.1
      self.W_hy = np.random.randn(hidden_size, 3) * 0.4

   The identity matrix component in ``W_hh`` helps maintain stability by preserving some of the previous hidden state.

.. dropdown:: Hint 2: Forward Step
   :class-title: sd-font-weight-bold

   For the ``forward_step`` method:

   .. code-block:: python

      h_new = np.tanh(np.dot(x, self.W_xh) + np.dot(h_prev, self.W_hh) + self.b_h)
      y = np.clip(np.dot(h_new, self.W_hy) + self.b_y, 0.0, 1.0)
      return y, h_new

.. dropdown:: Complete Solution
   :class-title: sd-font-weight-bold

   .. code-block:: python
      :linenos:

      import numpy as np
      from PIL import Image

      class MyRNN:
          def __init__(self, hidden_size=16):
              np.random.seed(42)
              self.hidden_size = hidden_size

              # Weight matrices
              self.W_xh = np.random.randn(3, hidden_size) * 0.3
              self.W_hh = np.eye(hidden_size) * 0.5 + np.random.randn(hidden_size, hidden_size) * 0.1
              self.W_hy = np.random.randn(hidden_size, 3) * 0.4
              self.b_h = np.zeros(hidden_size)
              self.b_y = np.array([0.5, 0.5, 0.5])

          def forward_step(self, x, h_prev):
              h_new = np.tanh(
                  np.dot(x, self.W_xh) +
                  np.dot(h_prev, self.W_hh) +
                  self.b_h
              )
              y = np.clip(np.dot(h_new, self.W_hy) + self.b_y, 0.0, 1.0)
              return y, h_new

          def generate_sequence(self, seed_color, length=20):
              h = np.zeros(self.hidden_size)
              colors = [np.array(seed_color)]
              current = seed_color

              for _ in range(length - 1):
                  next_color, h = self.forward_step(current, h)
                  colors.append(next_color)
                  current = next_color

              return np.array(colors)

      # Test and visualize
      rnn = MyRNN(hidden_size=16)
      sequence = rnn.generate_sequence([0.8, 0.2, 0.3], length=25)

      # Create visualization
      width, height = 500, 100
      img = np.zeros((height, width, 3), dtype=np.uint8)
      stripe_width = width // len(sequence)
      for i, color in enumerate(sequence):
          x_start = i * stripe_width
          x_end = (i + 1) * stripe_width if i < len(sequence) - 1 else width
          img[:, x_start:x_end] = (color * 255).astype(np.uint8)
      Image.fromarray(img).save('my_rnn_output.png')
      print("Saved: my_rnn_output.png")

**Challenge Extension**: Add a "temperature" parameter that controls how much the hidden state changes at each step. Higher temperature means more variation, lower temperature means smoother transitions.

.. dropdown:: Challenge Solution
   :class-title: sd-font-weight-bold

   .. code-block:: python

      def forward_step_with_temperature(self, x, h_prev, temperature=1.0):
          """Forward step with temperature control."""
          # Compute raw hidden state update
          h_raw = np.dot(x, self.W_xh) + np.dot(h_prev, self.W_hh) + self.b_h

          # Apply temperature to control dynamics
          # Higher temperature = more change, lower = smoother
          h_new = np.tanh(h_raw * temperature)

          y = np.clip(np.dot(h_new, self.W_hy) + self.b_y, 0.0, 1.0)
          return y, h_new

   Temperature scales the pre-activation values. With temperature < 1, the tanh operates closer to its linear region, creating smoother transitions. With temperature > 1, the tanh saturates more, creating more distinct states.


Summary
=======

Key Takeaways
-------------

* **Recurrent Neural Networks** process sequences by maintaining a hidden state that carries information across time steps
* The **hidden state** acts as memory, encoding a summary of all previously seen inputs
* The **recurrent cell** combines current input with previous state: h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)
* **Autoregressive generation** feeds outputs back as inputs to generate new sequences
* **Hidden size** determines memory capacity: larger states can store more complex patterns
* The **feedback connection** (W_hh) is what makes the network recurrent and enables sequential memory

Common Pitfalls
---------------

* **Forgetting hidden state initialization**: Always initialize h to zeros (or learned values) at sequence start
* **Unbounded values**: Without tanh or similar activation, hidden states can explode; always use non-linear activations
* **Not clipping outputs**: For color values, ensure outputs are in valid range [0, 1]
* **Confusing weight dimensions**: W_xh is (input_size, hidden_size), W_hh is (hidden_size, hidden_size), W_hy is (hidden_size, output_size)
* **Ignoring the recurrent connection**: The power of RNNs comes from W_hh; without it, you just have a feedforward network



References
==========

.. [Elman1990] Elman, J. L. (1990). Finding structure in time. *Cognitive Science*, 14(2), 179-211. https://doi.org/10.1207/s15516709cog1402_1 [Foundational paper introducing simple recurrent networks]

.. [Hochreiter1997] Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. *Neural Computation*, 9(8), 1735-1780. https://doi.org/10.1162/neco.1997.9.8.1735 [LSTM architecture solving vanishing gradients]

.. [Goodfellow2016b] Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*, Chapter 10: Sequence Modeling. MIT Press. https://www.deeplearningbook.org/ [Comprehensive RNN theory and practice]

.. [Graves2013] Graves, A. (2013). Generating sequences with recurrent neural networks. *arXiv preprint arXiv:1308.0850*. https://arxiv.org/abs/1308.0850 [Sequence generation techniques with RNNs]

.. [Karpathy2015] Karpathy, A. (2015). The unreasonable effectiveness of recurrent neural networks. *Blog post*. https://karpathy.github.io/2015/05/21/rnn-effectiveness/ [Practical char-RNN examples and intuition]

.. [LeCun2015b] LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. *Nature*, 521(7553), 436-444. https://doi.org/10.1038/nature14539 [Overview of deep learning including recurrent networks]

.. [Pascanu2013] Pascanu, R., Mikolov, T., & Bengio, Y. (2013). On the difficulty of training recurrent neural networks. *Proceedings of the 30th International Conference on Machine Learning*, 1310-1318. [Analysis of vanishing and exploding gradients]

.. [NumPyDocs923] NumPy Developers. (2024). NumPy linear algebra (numpy.dot). *NumPy Documentation*. https://numpy.org/doc/stable/reference/generated/numpy.dot.html

.. [PillowDocs923] Clark, A., et al. (2024). *Pillow: Python Imaging Library* (Version 10.2.0). Python Software Foundation. https://pillow.readthedocs.io/

.. [Galanter2016b] Galanter, P. (2016). Generative art theory. In C. Paul (Ed.), *A Companion to Digital Art* (pp. 146-180). Wiley-Blackwell. https://doi.org/10.1002/9781118475249.ch5 [Context for AI in generative art]
